{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVu0Llzmw_gz"
   },
   "source": [
    "## **1. Getting Started**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnF3BUAcs-us"
   },
   "source": [
    "### **1.1 Install Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nz_ZOyXx4oA"
   },
   "outputs": [],
   "source": [
    "# 2.1 Text Extraction\n",
    "!pip install textract\n",
    "\n",
    "# 3.2 LDA Mallet Model\n",
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "!unzip mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paa_G2Cuw2L1"
   },
   "source": [
    "### **1.2 Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVQyo5hqx-Mn"
   },
   "outputs": [],
   "source": [
    "# 2.1 Text Extraction\n",
    "import re\n",
    "import os\n",
    "import textract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# 2.2 Data Cleaning\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# 2.3 Creating Corpus\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# 2.4 Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 3.1 Word2Vec Model\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 3.2 LDA Mallet Model\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# 4.2 Top 10 Word Count\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "# 4.4 Bigram Analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# 4.5 Sentiment Analysis\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NZB7yHiw8ES"
   },
   "source": [
    "## **2. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaAIni6PxI-O"
   },
   "source": [
    "### **2.1 Text Extraction**\n",
    "*   Import PDF using Textract\n",
    "*   Clean up weird symbols, characters\n",
    "*   Split text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsEkbeDuxs82"
   },
   "outputs": [],
   "source": [
    "# function that takes in a file path and return a string of extracted text\n",
    "def read_pdf(file_path):\n",
    "    report = textract.process(file_path, method = 'pdfminer')\n",
    "    text = report.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPiHfAZHxvb7"
   },
   "outputs": [],
   "source": [
    "# function takes in a report and breaks it up into individual sentences\n",
    "def convert_pdf_into_sentences(text):\n",
    "\n",
    "    # remove unnecessary spaces and line breaks\n",
    "    text = re.sub(r'\\x0c\\x0c|\\x0c', \"\", str(text))\n",
    "    text = re.sub('\\n ', '', str(text))\n",
    "    text = re.sub('\\n', ' ', str(text))\n",
    "    text = ' '.join(text.split())\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    if \"”\" in text: text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "\n",
    "    #filter for sentences with more than 100 characters\n",
    "    sentences = [s.strip() for s in sentences if len(s) > 100]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StaaRlLTyOf2"
   },
   "outputs": [],
   "source": [
    "# function that combines all process for text extraction\n",
    "def extract_text(fi):\n",
    "\n",
    "    directory = 'data/type/' + fi\n",
    "\n",
    "    data = {'name': [], 'sentence': [], 'year': []}\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename[-3:] != \"pdf\":  # to filter out the README.md file\n",
    "            continue\n",
    "\n",
    "        companyname = filename.split(\"-\", 1)[0]\n",
    "        year = int(filename.split('-')[-1][:-4])\n",
    "\n",
    "        try:\n",
    "            text = read_pdf(directory + '/' + filename)\n",
    "            sentences = convert_pdf_into_sentences(text)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if len(sentences) == 0:\n",
    "            new_row = {'name': companyname, 'sentence': '', 'year': year}\n",
    "            data = data.append(new_row, ignore_index=True)\n",
    "      \n",
    "        else:\n",
    "            sent_arr = np.array(sentences)\n",
    "            shaped_array = np.reshape(sent_arr, len(sentences))\n",
    "            df = pd.DataFrame(shaped_array, columns = ['sentence'])\n",
    "            df.insert(0, column = 'name', value = companyname)\n",
    "            df['year'] = year\n",
    "            data = data.append(df, sort = False).reset_index(drop = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvMq0WFTylTg"
   },
   "outputs": [],
   "source": [
    "# Conduct text extraction \n",
    "asian_banks_sentences = extract_text('asian_banks')\n",
    "asian_banks_sentences['type'] = 'ab'\n",
    "\n",
    "asset_managers_sentences = extract_text('asset_managers')\n",
    "asset_managers_sentences['type'] = 'am'\n",
    "\n",
    "insurance_sentences = extract_text('insurance')\n",
    "insurance_sentences['type'] = 'ins'\n",
    "\n",
    "pension_funds_sentences = extract_text('pension_funds')\n",
    "pension_funds_sentences['type'] = 'pf'\n",
    "\n",
    "all_sentences = pd.concat([asian_banks_sentences, asset_managers_sentences, insurance_sentences, pension_funds_sentences], ignore_index = True)\n",
    "all_sentences = all_sentences.iloc[: , 1:] # remove index col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZyoKtqNxMsy"
   },
   "source": [
    "### **2.2 Text Cleaning**\n",
    "*   Gensim preprocessing\n",
    "*   Removing stopwords\n",
    "*   Create bigram models\n",
    "*   Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VyM4WUzzPcL"
   },
   "outputs": [],
   "source": [
    "# function for gensim preprocessing\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcTQaPsVzyI_"
   },
   "outputs": [],
   "source": [
    "# function for creating stopwords (english stopwords, context specific words, company names)\n",
    "def create_stopwords():\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    complabels = pd.read_csv('data/companylabels.csv', usecols=['fullname', 'shortform'])\n",
    "    ls_comp = complabels['fullname'].unique().tolist() + complabels['shortform'].unique().tolist()\n",
    "    for name in ls_comp:\n",
    "        for n in name.split(' '):\n",
    "            stop_words.append(n.lower())\n",
    "\n",
    "    stop_words.extend(['accounting', 'active', 'income', 'adventure', 'allocation', 'shares', 'amortization', 'amplitude', 'annuity', 'appreciation', 'arbitrage', 'ask', 'asset', 'asset approach', 'aval'])\n",
    "    stop_words.extend(['plc', 'group', 'target', 'track', 'capital', 'holding', 'report', 'annualreport', 'esg', 'bank', 'report', 'annualreport', 'long', 'make', 'fy'])\n",
    "    return stop_words\n",
    "\n",
    "# function for removing stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6l5hGATz2IP"
   },
   "outputs": [],
   "source": [
    "# function for creating bigram models\n",
    "def create_bigram_mod():\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases. \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "# function for making bigrams\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUsX2wD1z6oa"
   },
   "outputs": [],
   "source": [
    "# function for lemmatization\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD-KPaksz9F9"
   },
   "outputs": [],
   "source": [
    "# apply all functions (gensim preprocessing, stopwords removal, bigram creation, lemmatization)\n",
    "data = all_sentences.sentence.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "bigram_mod = create_bigram_mod()\n",
    "stop_words = create_stopwords()\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qov7AuUXxSmW"
   },
   "source": [
    "### **2.3 Creating Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhoOBH4i0fao"
   },
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized) # create dictionary\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts] # create corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy8WdhhYxXuc"
   },
   "source": [
    "### **2.4 Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FKAbeWqZqGg"
   },
   "outputs": [],
   "source": [
    "#Creation of bag of words model\n",
    "#Run generate_sentiment_score function below first, before running bag of words model\n",
    "def bag_of_words(df):\n",
    "    cv=CountVectorizer(max_features=1000)\n",
    "    df_final=cv.fit_transform(list(df['sentence'])).toarray()\n",
    "    vocab=cv.get_feature_names()\n",
    "    df2=pd.DataFrame(df_final,columns=vocab)\n",
    "    df2['optimism']=df['sentiment_score'].tolist()\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx2pHRkzxaXc"
   },
   "source": [
    "## **3. Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuRthxPd09gs"
   },
   "source": [
    "### **3.1 Word2Vec Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8I-F-ZT4yETq"
   },
   "outputs": [],
   "source": [
    "def word2vec(df,company_name,*args):\n",
    "    sent = [list(filter(lambda x:x not in stopwords.words('english'),row.split())) for row in df['sentence'].loc[df['name']==company_name]]\n",
    "    phrases = Phrases(sent, min_count=10)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = bigram[sent]\n",
    "\n",
    "    w2v_model = Word2Vec(min_count=1,\n",
    "                         window=3,\n",
    "                         size=55,\n",
    "                         sample=6e-5, \n",
    "                         alpha=0.05, \n",
    "                         min_alpha=0.0007, \n",
    "                         negative=20,\n",
    "                         )\n",
    "    w2v_model.build_vocab(sentences)\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=40, report_delay=1)\n",
    "\n",
    "    vocabs = []\n",
    "    vecs = []\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        vocabs.append(word)\n",
    "        vecs.append(w2v_model[word])\n",
    "\n",
    "    sub_vecs = vecs[:40]\n",
    "    sub_vocab = vocabs[:40]\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2000, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(sub_vecs)\n",
    "\n",
    "    for word in args:\n",
    "        return w2v_model.most_similar(word,topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2W2SFSfyUZS"
   },
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    arrays = np.empty((0,55), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "\n",
    "    reduc = PCA(n_components=21).fit_transform(arrays)\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzdMxpdhyehQ"
   },
   "outputs": [],
   "source": [
    "def word2vec_tsne(df,company_name,args):\n",
    "    sent = [list(filter(lambda x:x not in stopwords.words('english'),row.split())) for row in df['sentence'].loc[df['name']==company_name]]\n",
    "    phrases = Phrases(sent, min_count=10)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = bigram[sent]\n",
    "\n",
    "    w2v_model = Word2Vec(min_count=1,\n",
    "                         window=3,\n",
    "                         size=55,\n",
    "                         sample=6e-5, \n",
    "                         alpha=0.05, \n",
    "                         min_alpha=0.0007, \n",
    "                         negative=20,\n",
    "                         )\n",
    "    w2v_model.build_vocab(sentences)\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=40, report_delay=1)\n",
    "\n",
    "    vocabs = []\n",
    "    vecs = []\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        vocabs.append(word)\n",
    "        vecs.append(w2v_model[word])\n",
    "    print(vocabs)\n",
    "\n",
    "    sub_vecs = vecs[:40]\n",
    "    sub_vocab = vocabs[:40]\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2000, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(sub_vecs)\n",
    "    tsnescatterplot(w2v_model, args,  [i[0] for i in w2v_model.wv.most_similar(negative=['financial'])])\n",
    "\n",
    "word2vec_tsne(insurance_1, 'Taiwan Life', 'energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG_4AFPI1BMx"
   },
   "source": [
    "### **3.2 LDA Mallet Model**\n",
    "\n",
    "\n",
    "*   Build optimal LDA Mallet Model\n",
    "*   Find optimal number of topics & save optimal model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKaSweKu09AN"
   },
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9rSy1EQ1T6X"
   },
   "outputs": [],
   "source": [
    "# function to compute coherence value of each model\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T4ProWX1bFR"
   },
   "outputs": [],
   "source": [
    "# find optimal model and optimal number of topics for LDA model (highest coherence value)\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=2)\n",
    "max_value = max(coherence_values)\n",
    "max_index = coherence_values.index(max_value)\n",
    "topic_num = list(range(2, 21, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OgLxJp21ebQ"
   },
   "outputs": [],
   "source": [
    "# save optimal model and optimal number of topics\n",
    "optimal_model = model_list[max_index]\n",
    "optimal_num_topics = topic_num[max_index]\n",
    "\n",
    "# get topics and keywords from optimal model\n",
    "topics = optimal_model.show_topics(num_topics=optimal_num_topics, num_words=10,formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln4-uwxa1fuy"
   },
   "outputs": [],
   "source": [
    "print(\"Optimal Coherence Score: \" + str(max_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrCBY6a1xd40"
   },
   "source": [
    "## **4. Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejWTPXWB18WX"
   },
   "source": [
    "### **4.1 LDA model**\n",
    "\n",
    "*   Determine which topics from optimal model are decarbonization related\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4RC3ekhxWpW"
   },
   "outputs": [],
   "source": [
    "# function to determine whether a topic is related to decarbonization\n",
    "def find_e_topics(topics):\n",
    "    esg_words = ['carbon', 'footprint', 'clean', 'environment', 'esg', 'green', 'sustainability', \n",
    "                  'sustainable', 'energy', 'emission', 'climate', 'responsible', 'geothermal',\n",
    "                  'environmental','decarbonization', 'decarbonisation', 'greenhouse','renewable', 'ozone']\n",
    "\n",
    "    # Check for 'E' topics\n",
    "    e_topic = []\n",
    "    for n in range(len(topics)):\n",
    "        words = [x[0] for x in topics[n][1]]\n",
    "        for w in words:\n",
    "            if w in esg_words:\n",
    "                e_topic.append(n)\n",
    "                break\n",
    "\n",
    "    return e_topic  # returns topic number that are related to decarbonization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfuVDYL52pf5"
   },
   "source": [
    "*   Assign dominant topic to each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlLXRL_Z2KP1"
   },
   "outputs": [],
   "source": [
    "# function to assign dominant topic\n",
    "def format_topics_sentences(ldamodel, corpus=corpus, texts=data):\n",
    "  \n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVILXkN72Muq"
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['document_no', 'dominant_topic', 'topic_perc_contrib', 'keywords', 'text']\n",
    "\n",
    "# assign dominant topic to each sentence\n",
    "all_sentences['dominant_topic'] = df_dominant_topic['dominant_topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMIaurYI2tkY"
   },
   "source": [
    "*   Determine which sentences are decarbonization related\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4Ud_mYc2Vx1"
   },
   "outputs": [],
   "source": [
    "# function that determines which sentences are decarbonization related\n",
    "def assign_esg(df):\n",
    "    e_topic = find_e_topics(topics)\n",
    "    new_df = df\n",
    "    new_df['esg'] = \"\"\n",
    "    for i, row in df.iterrows():\n",
    "        topic = new_df.iloc[i, new_df.columns.get_loc('dominant_topic')]\n",
    "        if topic in e_topic:\n",
    "            new_df.at[i, 'esg'] = \"E\"\n",
    "        else:\n",
    "            new_df.at[i, 'esg'] = \"SG\"\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntkRfmhN2eEv"
   },
   "outputs": [],
   "source": [
    "df_with_assigned_topics = assign_esg(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM0l1aMf2w3K"
   },
   "source": [
    "*   Extract all decarbonization related sentences for bigram analysis (dashboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aBj3ysm2fWf"
   },
   "outputs": [],
   "source": [
    "# extract decarbonization related sentences for bigram analysis\n",
    "all_e_sentences = df_with_assigned_topics[df_with_assigned_topics['esg'] == \"E\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsxj9GF029r6"
   },
   "source": [
    "*   Calculate percentage of decarbonization related sentences for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAGzWVLv2-XR"
   },
   "outputs": [],
   "source": [
    "# to calculate percentage\n",
    "all_nrows = df_with_assigned_topics.groupby(\"name\").size().reset_index(name='counts')\n",
    "all_nrows.rename(columns={\"counts\": \"total_sent\"}, inplace = True)\n",
    "e_score = all_e_sentences.groupby(\"name\").size().reset_index(name='counts')\n",
    "e_score.rename(columns={\"counts\": \"e_sent\"}, inplace = True)\n",
    "all_percent = all_nrows.merge(e_score, how=\"left\")\n",
    "all_percent['percent'] = all_percent['e_sent']/all_percent['total_sent'] * 100\n",
    "all_percent.fillna(0, inplace = True)\n",
    "comp_fi = all_sentences[['name', 'type']].drop_duplicates()\n",
    "all_percent = all_percent.merge(comp_fi, how='left')\n",
    "all_percent.to_csv('all_percent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5Zxc78v3vqS"
   },
   "source": [
    "### **4.2 Top 10 Word Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tfT1-hg30EG"
   },
   "outputs": [],
   "source": [
    "df = all_sentences\n",
    "df['words_lemmatized'] = data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVCgiwQP4LLT"
   },
   "outputs": [],
   "source": [
    "def create_dict(df):\n",
    "    comp_word = {}\n",
    "    for i, row in df.iterrows():\n",
    "        comp = df.iloc[i, df.columns.get_loc('name')]\n",
    "        ls_words = df.iloc[i, df.columns.get_loc('words_lemmatized')]\n",
    "\n",
    "        counts = Counter(ls_words)\n",
    "\n",
    "        if comp not in comp_word:\n",
    "            comp_word[comp] = dict(counts)\n",
    "        else:\n",
    "            curr = Counter(comp_word[comp])\n",
    "            new = dict(curr + counts)\n",
    "            comp_word[comp] = new\n",
    "\n",
    "    return comp_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYColLYk4gn-"
   },
   "outputs": [],
   "source": [
    "def top_10(comp_dict):\n",
    "    df = pd.DataFrame(columns = ['name'])\n",
    "    for key, value in comp_dict.items():\n",
    "        top10 = dict(sorted(value.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "        new = pd.DataFrame({'name': key, 'top10': top10}).reset_index()\n",
    "        new = new.rename(columns={'top10': 'count','index': 'bigram'})\n",
    "\n",
    "        if key in ab:\n",
    "            new['type'] = 'ab'\n",
    "        elif key in am:\n",
    "            new['type'] = 'am'\n",
    "        elif key in ins:\n",
    "            new['type'] = 'ins'\n",
    "        elif key in pf:\n",
    "            new['type'] = 'pf'\n",
    "        df = df.append(new)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld6DiGaP4i61"
   },
   "outputs": [],
   "source": [
    "word_count_dict = create_dict(df)\n",
    "all_word_count_top10 = top_10(word_count_dict)\n",
    "all_word_count_top10.reset_index(inplace=True)\n",
    "all_word_count_top10.drop('index', axis = 1, inplace=True)\n",
    "all_word_count_top10.to_csv('all_word_count_top10.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmv46jOB5M-Q"
   },
   "source": [
    "### **4.3 Extract Global Initiatives & Standards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EijFg72_5RuG"
   },
   "outputs": [],
   "source": [
    "def extract_initiatives(data):\n",
    "    # import list of initiatives\n",
    "    initiatives_file = pd.read_csv('data/esg_initiatives.csv', usecols=['Initiative'])\n",
    "    initiatives = initiatives_file['Initiative'].tolist()\n",
    "\n",
    "    companies = data['name'].unique().tolist()\n",
    "\n",
    "    compiled_initiatives = []\n",
    "\n",
    "    for company in companies: \n",
    "        partial_df = data.loc[data['name'] == company] #change type of FI accordingly (X2)\n",
    "        type_of_fi = partial_df['type'].unique().tolist()[0]\n",
    "        filtered_initiatives = []\n",
    "\n",
    "    for sentence in partial_df['sentence']:\n",
    "        sentence = str(sentence)\n",
    "        for initiative in initiatives:\n",
    "            if initiative in sentence and initiative not in filtered_initiatives: \n",
    "                filtered_initiatives.append(initiative)\n",
    "\n",
    "        # To standardise the naming convention of HK Stock Exchange & Stock Exchange of HK\n",
    "        if 'Hong Kong Stock Exchange' in filtered_initiatives:\n",
    "            filtered_initiatives.remove('Hong Kong Stock Exchange')\n",
    "            if 'Stock Exchange of Hong Kong' not in filtered_initiatives:\n",
    "                filtered_initiatives.append('Stock Exchange of Hong Kong')\n",
    "        count = len(filtered_initiatives)\n",
    "        compiled_initiatives.append([company, filtered_initiatives, count, type_of_fi])\n",
    "\n",
    "    initiatives_df = pd.DataFrame(compiled_initiatives, columns = ['name', 'initiatives', 'count', 'type'])\n",
    "    return initiatives_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2pZ8Xkk5ZCG"
   },
   "outputs": [],
   "source": [
    "all_initiatives = extract_initiatives(all_sentences)\n",
    "all_initiatives.to_csv('all_initiatives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgXcMucw6eBh"
   },
   "source": [
    "### **4.4 Bigram Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zl-x9Cwd6uwh"
   },
   "outputs": [],
   "source": [
    "#load spacy model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "def tokenize(sentence):\n",
    "    gen = gensim.utils.simple_preprocess(sentence, deacc=True)\n",
    "    return ' '.join(gen)\n",
    "\n",
    "def lemmatize(text):\n",
    "  \n",
    "    # parse sentence using spacy\n",
    "    doc = nlp(text) \n",
    "\n",
    "    # convert words into their simplest form (singular, present form, etc.)\n",
    "    lemma = []\n",
    "    for token in doc:\n",
    "        if (token.lemma_ not in ['-PRON-']):\n",
    "            lemma.append(token.lemma_)\n",
    "\n",
    "    return tokenize(' '.join(lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIMClkT360L4"
   },
   "outputs": [],
   "source": [
    "all_e_sentences = all_e_sentences.replace({np.nan: '-'})\n",
    "all_e_sentences['lemma'] = all_e_sentences['sentence'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ez_kOJ6z62Hw"
   },
   "outputs": [],
   "source": [
    "def create_stopwords_for_bigram(): \n",
    "    # context specific keywords not to include in topic modelling\n",
    "    context_stopwords = ['report', 'annualreport', 'pdf', 'firm', 'company', 'uobam', 'page', 'ceo', \n",
    "                      'content', 'index', 'guide', 'data', 'chairman', 'executive', 'chief', 'fiscal',\n",
    "                      'stakeholders', 'acn', 'vs', 'yoy', 'brigade', 'station', 'site', 'table',\n",
    "                      'journey', 'achieve', 'endure', 'reporting', 'period', 'head', 'way', 'gri', \n",
    "                      'holding', 'http', 'https', 'www', 'hi', 'audit', 'fy']\n",
    "\n",
    "    country_stopwords = ['malaysia', 'hong', 'kong', 'china', 'country', 'region', 'japan', 'japanese', \n",
    "                      'tokyo', 'south', 'africa', 'france', 'switzerland', 'germany', 'rio', \n",
    "                      'kuala', 'lumpur', 'hk', 'hkex', 'australia', 'eu', 'chinese', 'mainland',\n",
    "                      'citi', 'pacific', 'mitsubishi', 'cid', 'cppib', 'dai', 'chi']\n",
    "\n",
    "    currency_stopwords = ['eur', 'million', 'es', 'dd', 'source', 'rmb', 'krw', 'trillion', 'billion', 'euro']                    \n",
    "\n",
    "    date_stopwords = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', \n",
    "                    'september', 'october', 'november', 'december', 'year', 'month', 'annual']\n",
    "\n",
    "    stopwords = context_stopwords + country_stopwords + date_stopwords + currency_stopwords\n",
    "\n",
    "    # add company names as stop words\n",
    "    complabels = pd.read_csv('data/companylabels.csv', usecols=['fullname', 'shortform'])\n",
    "    ls_comp = complabels['fullname'].unique().tolist() + complabels['shortform'].unique().tolist()\n",
    "    for name in ls_comp:\n",
    "        for n in name.split(' '):\n",
    "            stop_words.append(n.lower())\n",
    "\n",
    "    # our list contains all english stop words + companies names + specific keywords\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hymqRMwK64dR"
   },
   "outputs": [],
   "source": [
    "def bigram(sub_df): \n",
    "\n",
    "    bigram_stopwords = create_stopwords_for_bigram()\n",
    "\n",
    "    # Run bi-gram TF-IDF frequencies\n",
    "    bigram_tf_idf_vectorizer = TfidfVectorizer(stop_words=bigram_stopwords, ngram_range=(2,2), min_df=1, use_idf=True)\n",
    "    bigram_tf_idf = bigram_tf_idf_vectorizer.fit_transform(sub_df.lemma)\n",
    "\n",
    "    # Extract bi-grams names\n",
    "    words = bigram_tf_idf_vectorizer.get_feature_names()\n",
    "\n",
    "    # extract our top 10 ngrams\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in bigram_tf_idf:\n",
    "        total_counts += t.toarray()[0]\n",
    "\n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUfgIIhq8lOi"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "companies = all_e_sentences['name'].unique().tolist()\n",
    "for c in companies:\n",
    "    sub_df = all_e_sentences.loc[all_e_sentences['name'] == c]\n",
    "    bigram_dict = bigram(sub_df) \n",
    "    data.append([c, bigram_dict])\n",
    "\n",
    "bigram_df = pd.DataFrame(data, columns = ['name', 'bigramarray'])\n",
    "bigram_df.to_csv('bigram_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xF-6JU_9TUp"
   },
   "source": [
    "### **4.5 Calculate Sentiment Score** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Orq3GyT-9dx0"
   },
   "outputs": [],
   "source": [
    "def generate_sentiment_score(df):\n",
    "    df['sentence']=[str(x).lower() for x in df['sentence']]\n",
    "    df['tokenize']=[x.split(' ') for x in df['sentence']]\n",
    "    df['sentiment']=df['sentence'].apply(lambda x:TextBlob(x).sentiment.polarity)\n",
    "    optimism=[]\n",
    "\n",
    "    esg_keywords = ['best-in-class', 'carbon footprint', 'carbon pricing', 'clean technology', 'engagement', 'environmental factors', 'esg integration', 'ethical investing', 'exclusions', 'negative screening', 'governance factors', 'green bond', 'greenwashing', 'human rights', 'impact investments', 'modern slavery', 'PRI', 'proxy voting', \n",
    "                'renewable energy', 'screening', 'social factors', 'SRI', 'stewardship', 'thematic investing', 'SDG', 'values-based investing', 'voting rights', 'biodiversity', 'carbon capture and storage', 'circular economy', 'climate action tracker', 'climate clocks', 'climate funds', \n",
    "                'climate transition benchmarks', 'greenhouse gas emissions', 'net zero carbon pledge and initiative', 'paris agreement', 'paris-aligned benchmarks', \n",
    "                'PFAS', 'scope 1', 'scope 2', 'scope 3', 'sdg funds', 'sin stocks', 'smart esg scores', 'social sustainability', 'stewardship code', 'stranded assets', 'sustainable investing', 'sustainability reporting', \n",
    "                'sustainable supply chains', 'sustainable technology', 'thermal coal exposure', 'triple bottom line', 'un global impact','green','low-carbon']\n",
    "    lst=['acidification','biofuel','carbon','carbon dioxide','climate','co2','climate change','decarbonisation','decarbonization','energy transmission','energy','energy transition','energy storage','emissions','emission control','fossil fuels','geothermal energy','geothermal','greenhouse gas','greenhouse','hydrocarbons','LNG','liquefied natural gas','ozone','renewable resources','sng','synthetic natural gas','thermal energy','thermal','wind power','wind']\n",
    "    empty=[]\n",
    "    all_keyword=[]\n",
    "    word_collection=[]\n",
    "    for i in df['sentence']:\n",
    "        dic={}\n",
    "        total=0\n",
    "        keyword=[]\n",
    "        for word in esg_keywords+lst:\n",
    "            for j in range(0,len(i)):\n",
    "                if i[j:j+len(word)]==word and word not in keyword:\n",
    "                    total+=1\n",
    "                    keyword.append(word)\n",
    "                    dic[word]=1\n",
    "                elif i[j:j+len(word)]==word:\n",
    "                    total+=1\n",
    "                    dic[word]+=1\n",
    "        all_keyword.append(keyword)\n",
    "        empty.append(total)\n",
    "        word_collection.append(dic)\n",
    "\n",
    "    df['freq']=empty      \n",
    "    df['unique_keywords']=all_keyword\n",
    "    df['words_collection']=word_collection\n",
    "\n",
    "    score=[]\n",
    "    for index,rows in df.iterrows(): \n",
    "        if rows['freq']==0:\n",
    "            score.append(rows['sentiment'])\n",
    "        else:\n",
    "            score.append(rows['sentiment']+math.log(1+rows['freq']*len(rows['unique_keywords'])))\n",
    "    df['sentiment_score']=score\n",
    "    for i in df['sentiment_score']:\n",
    "        if i<0:\n",
    "            optimism.append('pessimistic')\n",
    "        elif i<0.5:\n",
    "            optimism.append('neutral')\n",
    "        else:\n",
    "            optimism.append('optimistic')\n",
    "    df['optimism']=optimism\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H53gbR76bygM"
   },
   "outputs": [],
   "source": [
    "asian_banks_1=generate_sentiment_score(asian_banks_sentences)\n",
    "pension_funds_1=generate_sentiment_score(pension_funds_sentences)\n",
    "asset_managers_1=generate_sentiment_score(asset_managers_sentences)\n",
    "insurance_1=generate_sentiment_score(insurance_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpK9OFhlcLCh"
   },
   "outputs": [],
   "source": [
    "#Generate Bag Of Words Model for each data frame\n",
    "bow_asian_bank=bag_of_words(asian_banks_1)\n",
    "bow_pension_funds=bag_of_words(pension_funds_1)\n",
    "bow_insurance=bag_of_words(insurance_1)\n",
    "bow_asset_managers=bag_of_words(asset_managers_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOoBlwmqaK8M"
   },
   "outputs": [],
   "source": [
    "#Regression Tree For predicting future sentiment score\n",
    "def decision_tree_final(df2):\n",
    "    tree_model=tree.DecisionTreeRegressor()\n",
    "    x_train_1,x_test_1,y_train_1,y_test_1=train_test_split(df2.drop(columns=['optimism']),df2[['optimism']],train_size=0.7,test_size=0.3,random_state=1)\n",
    "    tree_model.fit(x_train_1,y_train_1)\n",
    "    tree_model.predict(x_test_1)\n",
    "    print('The root mean squared error of Regression Tree is {}'.format(np.sqrt(mean_squared_error(y_test_1,tree_model.predict(x_test_1)))))\n",
    "    x_test_1['predicted_optimism']=tree_model.predict(x_test_1).tolist()\n",
    "    x_train_1['predicted_optimism']=tree_model.predict(x_train_1).tolist()\n",
    "    df1=pd.concat([x_test_1,y_test_1],axis=1)\n",
    "    df3=pd.concat([x_train_1,y_train_1],axis=1)\n",
    "    return pd.concat([df1,df3],axis=0)\n",
    "\n",
    "#Input Bag Of Words\n",
    "\n",
    "#Linear Regression for predicting future sentiment score\n",
    "\n",
    "def linear_regression(df2):\n",
    "    lr=LinearRegression()\n",
    "    x_train_1,x_test_1,y_train_1,y_test_1=train_test_split(df2.drop(columns=['optimism']),df2[['optimism']],train_size=0.7,test_size=0.3,random_state=1)\n",
    "    lr.fit(x_train_1,y_train_1)\n",
    "    lr.predict(x_test_1)\n",
    "    print('The root mean squared error of Linear Regression is {}'.format(np.sqrt(mean_squared_error(y_test_1,lr.predict(x_test_1)))))\n",
    "    x_test_1['predicted_optimism']=lr.predict(x_test_1)\n",
    "    x_train_1['predicted_optimism']=lr.predict(x_train_1)\n",
    "    df1=pd.concat([x_test_1,y_test_1],axis=1)\n",
    "    df3=pd.concat([x_train_1,y_train_1],axis=1)\n",
    "    return pd.concat([df1,df3],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYuWhCBrcYrN"
   },
   "outputs": [],
   "source": [
    "asian_bank_sentiment=decision_tree_final(bow_asian_bank).sort_index()\n",
    "insurance_sentiment=decision_tree_final(bow_insurance).sort_index()\n",
    "asset_managers_sentiment=decision_tree_final(bow_asset_managers).sort_index()\n",
    "pension_funds_sentiment=decision_tree_final(bow_pension_funds).sort_index()\n",
    "\n",
    "insurance_sentiment_lr=linear_regression(bow_insurance).sort_index()\n",
    "asset_managers_sentiment_lr=linear_regression(bow_asset_managers).sort_index()\n",
    "pension_funds_sentiment_lr=linear_regression(bow_pension_funds).sort_index()\n",
    "asian_banks_sentiment_lr=linear_regression(bow).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SxBjHIndwoC"
   },
   "outputs": [],
   "source": [
    "insurance_1['predicted_sentiment_tree']=insurance_sentiment['predicted_optimism']\n",
    "asset_managers_1['predicted_sentiment_tree']=asset_managers_sentiment['predicted_optimism']\n",
    "pension_funds_1['predicted_sentiment_tree']=pension_funds_sentiment['predicted_optimism']\n",
    "asian_banks_1['predicted_sentiment_tree']=asian_banks_sentiment['predicted_optimism']\n",
    "\n",
    "insurance_1['predicted_sentiment_lr']=insurance_sentiment_lr['predicted_optimism']\n",
    "asset_managers_1['predicted_sentiment_lr']=asset_managers_sentiment_lr['predicted_optimism']\n",
    "pension_funds_1['predicted_sentiment_lr']=pension_funds_sentiment_lr['predicted_optimism']\n",
    "asian_banks_1['predicted_sentiment_lr']=asian_banks_sentiment_lr['predicted_optimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fqbk8_32eeK4"
   },
   "outputs": [],
   "source": [
    "#Computation of current average sentiment scores for each company\n",
    "df_insurance=pd.DataFrame(insurance_1.groupby(['name'])['sentiment_score'].mean().reset_index())\n",
    "df_insurance['type']=\"ins\"\n",
    "\n",
    "df_asset_managers=pd.DataFrame(asset_managers_1.groupby(['name'])['sentiment_score'].mean().reset_index())\n",
    "df_asset_managers['type']=\"am\"\n",
    "\n",
    "df_pension_funds=pd.DataFrame(pension_funds_1.groupby(['name'])['sentiment_score'].mean().reset_index())\n",
    "df_pension_funds['type']=\"pf\"\n",
    "\n",
    "df_asian_banks=pd.DataFrame(asian_banks_1.groupby(['name'])['sentiment_score'].mean().reset_index())\n",
    "df_asian_banks['type']=\"ab\"\n",
    "\n",
    "all_df=pd.concat([df_asian_banks,df_asset_managers,df_pension_funds,df_insurance],0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YJOFJQPfHg-"
   },
   "outputs": [],
   "source": [
    "#Computation of average predicted sentiment scores for each company using Regression Tree\n",
    "df_insurance_new=pd.DataFrame(insurance_1.groupby(['name'])['predicted_sentiment_tree'].mean().reset_index())\n",
    "df_insurance_new['type']=\"ins\"\n",
    "\n",
    "df_asset_managers_new=pd.DataFrame(asset_managers_1.groupby(['name'])['predicted_sentiment_tree'].mean().reset_index())\n",
    "df_asset_managers_new['type']=\"am\"\n",
    "\n",
    "df_pension_funds_new=pd.DataFrame(pension_funds_1.groupby(['name'])['predicted_sentiment_tree'].mean().reset_index())\n",
    "df_pension_funds_new['type']=\"pf\"\n",
    "\n",
    "df_asian_banks_new=pd.DataFrame(asian_banks_1.groupby(['name'])['predicted_sentiment_tree'].mean().reset_index())\n",
    "df_asian_banks_new['type']=\"ab\"\n",
    "\n",
    "all_df_new=pd.concat([df_asian_banks_new,df_asset_managers_new,df_pension_funds_new,df_insurance_new],0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlzCV0rWgQBb"
   },
   "outputs": [],
   "source": [
    "#Computation of average predicted sentiment scores for each company using Linear Regression\n",
    "df_insurance_new_lr=pd.DataFrame(insurance_1.groupby(['name'])['predicted_sentiment_lr'].mean().reset_index())\n",
    "df_asset_managers_new_lr=pd.DataFrame(asset_managers_1.groupby(['name'])['predicted_sentiment_lr'].mean().reset_index())\n",
    "df_pension_funds_new_lr=pd.DataFrame(pension_funds_1.groupby(['name'])['predicted_sentiment_lr'].mean().reset_index())\n",
    "df_asian_banks_new_lr=pd.DataFrame(asian_banks_1.groupby(['name'])['predicted_sentiment_lr'].mean().reset_index())\n",
    "all_df_new_lr=pd.concat([df_asian_banks_new_lr,df_asset_managers_new_lr,df_pension_funds_new_lr,df_insurance_new_lr],0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdAPFGhpgeLq"
   },
   "outputs": [],
   "source": [
    "final_df=pd.concat([all_df_new,all_df_new_lr['predicted_sentiment_lr']],axis=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vVu0Llzmw_gz",
    "1NZB7yHiw8ES",
    "UaAIni6PxI-O",
    "iZyoKtqNxMsy",
    "qov7AuUXxSmW",
    "sy8WdhhYxXuc",
    "Gx2pHRkzxaXc",
    "zuRthxPd09gs",
    "JG_4AFPI1BMx",
    "VrCBY6a1xd40",
    "ejWTPXWB18WX",
    "E5Zxc78v3vqS",
    "Pmv46jOB5M-Q",
    "BgXcMucw6eBh",
    "3xF-6JU_9TUp"
   ],
   "name": "BT4103_Decarbonization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
